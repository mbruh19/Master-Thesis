
\subsection{Code Organization}

One of the major challenges of this thesis has been to develop the framework for training BNNs and TNNs. The framework needed to be quite flexible so that it allowed both BNNs and TNNs but also such that all of the algorithms and experiments could be tested within the same framework. All of the source code for this thesis can be found at: (GitHub link). The implementation is done in Python. To train a BNN or TNN, 'main.py' needs to be called with the right parameters. From this file, everything else runs automatically. As a starting point, it initializes the 'Reader' class and loads the training, validation and testing set by calling the load data function. The current framework support loading the MNIST, Fashion-MNIST (FMNIST) and the Adult dataset. To load from other datasets, the necessary function need to be implemented in the Reader class. Next, the 'Instance' class is called, which takes all the settings of the experiment as input as well as the three datasets. The Instance class has the very important 'loader' function, needed to iterate through batches from one of the datasets. \\

\noindent Having this, one of the algorithms presented is called. When an algorithm is called, it initially starts by loading a batch and create a 'Solution' object. This object is the core of the implementation. The most important attributes of this object is the $W$, $S$ and $U$ matrices introduced earlier as well as a vector, $O$ denoting the contribution of each sample to the objective function. It also has an attribute of the nodes in the network, that can be iterated through. The $S$, $U$ and $O$ attributes are dependent on the current batch and as a result the Solution object has a function to change the batch, which includes re-evaluating these attributes. The object also has functions to initialize a random solution and copy the weights. More importantly, it is also here the function to commit a move is placed, where the necessary updates are done. The remaining functions are functions to perturb the solution, used in ILS, and a function to select search weights, used in sporadic local search. \\

\noindent The algorithm then moves on and calls one the 'solvers' implemented. There are two 'solvers' implemented, iterated improvement which follows Algorithm \ref{iterated_improvement} or iterated local search, described in Algorithm \ref{ils}, which uses Algorithm \ref{iterated_improvement}. In the iterated improvement algorithm, the 'Delta Manager' object is called. This object has a single function, delta calculation, which either use a delta function for BNN or TNN. This function takes the current solution and a node as input. For this node, all the weights going into this node, which are part of the search, is tested to see what the delta value is if the weight changed value. This follows the procedure described in section 5.4. The delta calculation function returns a sequence of moves back to the iterated improvement algorithm, which takes the best of the moves and decide whether to commit the move or not. If the algorithm uses multiple batch training, it loads a new batch, change the batch on the solution and the process repeats itself until the time limit has been reached. \\