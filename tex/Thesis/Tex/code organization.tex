
\subsection{Code Organization}

One of the major challenges of this thesis has been to develop the framework for training BNNs and TNNs. The framework needed to be quite flexible so that it allows both BNNs and TNNs, but also ensures that all of the algorithms and experiments can be tested within the same framework. All of the source code for this thesis can be found at: https://github.com/mbruh19/Master-Thesis. The implementation is done in Python. To train a BNN or TNN, 'main.py' needs to be called with the right parameters. From this file, everything else runs automatically. As a starting point, it initializes the 'Reader' class and loads the training, validation, and testing sets by calling the load data function. The current framework supports loading the MNIST, Fashion-MNIST (FMNIST) and the Adult dataset. To load from other datasets, the necessary function needs to be implemented in the Reader class. Next, the 'Instance' class is called, which takes all the settings of the experiment as input as well as the three datasets. The Instance class has the very important 'loader' function, needed to iterate through batches from one of the datasets. \\

\noindent Having this, one of the algorithms presented is called. When an algorithm is called, it initially starts by loading a batch and creating a 'Solution' object. This object is the core of the implementation. The most important attributes of this object are the $W$, $S$ and $U$ matrices introduced earlier as well as a vector, $O$ denoting the contribution of each sample to the objective function. It also has an attribute of the nodes in the network, that can be iterated through. The $S$, $U$ and $O$ attributes are dependent on the current batch, and as a result, the Solution object has a function to change the batch, which includes re-evaluating these attributes. The object also has functions to initialize a random solution and copy the weights. More importantly, it is also here the function to commit a move is located, where the necessary updates are done. The remaining functions are those to perturb the solution, used in ILS, and a function to select search weights, used in sporadic local search. \\

\noindent The algorithm then moves on and calls one of the 'solvers' implemented. There are two 'solvers' implemented, iterated improvement, which follows Algorithm \ref{iterated_improvement} or iterated local search, described in Algorithm \ref{ils}, which uses Algorithm \ref{iterated_improvement}. In the iterated improvement algorithm, the 'Delta Manager' object is called. This object has a single function, delta calculation, which either uses a delta function for BNN or TNN. This function takes the current solution and a node as input. For this node, all the weights going into this node, which are part of the search, are tested to see what the delta value is if the weight changes value. This follows the procedure described in section 5.4. The delta calculation function returns a sequence of moves back to the iterated improvement algorithm, which takes the best of the moves and decides whether to commit the move or not. If the algorithm uses multiple batch training, it loads a new batch, changes the batch on the solution object. and the process repeats itself until the time limit has been reached. \\