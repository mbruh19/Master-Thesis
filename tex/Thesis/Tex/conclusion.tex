\section{Conclusion and Future Work}
In this thesis, I investigated the feasibility of training binary and ternary neural networks using local search methods. By efficiently utilizing the discrete nature of these networks, I implemented an efficient delta evaluation, enabling the training of networks within small time limits compared to existing MIP methods for training few-bits neural networks. \\

\noindent Existing work on training binary and ternary neural networks using MIP models has primarily focused on few-shot learning with small network architectures due to their high training time, which limits scalability. In contrast, the local search procedure presented in this thesis scales well with increased data and larger networks. For instance, I demonstrated that the BeMi ensemble introduced by \cite{ambrogio2023} can achieve higher accuracy than previously reported by training on more data, making it very suitable for few-shot learning. Specifically, with 1,000 training examples, the emsemble achieved a mean accuracy of 86.84\% on the MNIST dataset. This significant improvement over the 81\% they reported can be attributed to two main factors: the use of additional data and the omission of one of the two hidden layers, which widened the range of the preactivation values. The 86.84\% achieved with the BeMi ensemble also surpasses the results from training on a single batch of data with 2,000 training examples and a network with a single hidden layer of 128 neurons, where the maximum mean accuracy was only 81.85 \%. The BeMi ensemble outperforms this result with half the data, and a similar pattern is observed on the more challenging Fashion-MNIST dataset. \\

\noindent In addition to focusing on few-shot learning, this work proposes several algorithms capable of training on multiple batches, thereby enabling the use of much larger datasets. Among these, the aggregation algorithm showed great potential. Unlike the other proposed methods that adjust weights based on a single batch of samples, this algorithm calculates delta values for multiple batches, aggregates them, and adjusts the weights based on the aggregated values. This approach helps ensure better generalization of the model. The aggregation algorithm facilitated the training of larger networks, where the proposed integer objective function outperformed the standard cross-entropy loss function and the Brier score objective function. Specifically, the integer objective function demonstrated reduced overfitting compared to the other objective functions. However, a notable drawback is that this objective function does not always perform well with small network architectures, as it does not handle probabilities in the same way as the other functions. \\

\noindent The aggregation algorithm, combined with the integer objective function, achieved a test accuracy of 91.53\% on the MNIST dataset using a neural network with two hidden layers, each containing 128 neurons. While this result is promising, it is important to note that a significant gap remains compared to state-of-the-art binary neural network training methods that employ more traditional approaches. Despite this gap, the success of the aggregation algorithm and integer objective function in reaching high accuracy with relatively simple network architectures demonstrates their potential. These methods offer an alternative approach that can be particularly advantageous in scenarios where computational resources are limited. \\

\noindent With the increasing use of AI in real-world applications, binary and ternary neural networks are gaining significant attention. These networks are particularly valuable in computationally limited and energy-constrained devices. Binary and ternary neural networks show great potential in these environments due to their reduced computational complexity and lower power consumption compared to traditional floating-point neural networks. However, a performance gap often exists when working with discrete neural networks compared to their floating-point counterparts. This performance gap is also observed in this thesis. Nevertheless, the local search framework developed provides a promising foundation for narrowing this gap. \\

\noindent Future work can go in several directions. While this implementation uses Python, it would be interesting to investigate whether the running time could be reduced by implementing the algorithms in a language supporting more optimized code development, such as C++. C++ is closer to the hardware and it is in general faster than Python, but it does not have the same large standard libraries as Python and is considered more difficult to learn than Python. \\

\noindent It is also possible that the proposed algorithms, such as the aggregation algorithm can be expanded further. It would be interesting to investigate whether a perturbation strategy could help the model get to an even better solution. It might also be the case that the algorithm takes too many moves at once. The delta evaluation is always calculated under the assumption that it is only the value of that weight that changes. The aggregation algorithms commit many moves at once and as a result it is a possibility, especially later on in the algorithm, that the committed moves in a way cancels each other out. \\

\noindent As mentioned, the delta evaluation assumes only one weight changes at a time. In a local search context, this is known as the one-move-neighborhood. This work has not explored the use of larger neighborhoods, where the delta evaluation considers simultaneous changes in multiple weights. Such a strategy would significantly increase the neighborhood size and as a result it would be necessary to come up with heuristics for choosing weights that will be beneficial to change simultaneously. \\

\noindent It could also be worth investigating whether other regularization strategies could help reduce the overfitting gap. An often used regularization strategy in deep learning is dropout, where neurons during training are randomly deactivated with some probability. For each example in a batch, a different subset of the neurons is deactivated, ensuring that the same neurons are not always left out for the entire batch. This approach traditionally help preventing the network from becoming too dependent on specific neurons, thus improving generalization and robustness. \\ 

\noindent This work is only concerned with the training of simple feed-forward neural networks. It would be interesting to see if it is also feasible and scalable to train other types of neural networks. Convolutional neural networks are often used in image classification, as they learn features by themselves via their filters. \cite{lin2017} has already shown that it is possible to achieve good results using filters with values restricted to 1 or -1, while still employing real-valued weights during training. A natural extension of this work would be to test if it is possible to train binary convolutional neural networks using local search as well. 



