\section{Introduction}
In recent years, deep learning (DL) has revolutionized various fields such as image recognition \citep{imagenet2012}, natural language processing \citep{attention2017}, and speech recognition \citep{hinton2012}. DL uses neural networks (NNs), which have the ability to learn and model complicated patterns in data. Despite their success, traditional NNs face significant challenges when deployed on resource-constrained devices. Traditional NNs use floating-point representations, and as a result, the computational costs are high, there is an extensive memory requirement, and significant energy consumption. \\ 

\noindent An alternative to traditional NNs is binary and ternary neural networks (BNNs and TNNs), which reduce the values of the weights to binary or ternary values. This reduction can lead to substantial improvements in efficiency and lower power consumption. This makes these networks particularly suitable for use on low-power devices. Despite their huge potential, training BNNs and TNNs remains a complex task due to the discrete nature of their weights and activations. \\

\noindent BNNs and TNNs have attracted attention from the mixed-integer programming (MIP) community, and several papers have been published with MIP models aimed at training few-bits NNs \citep{icarte2019}, \citep{ambrogio2023} and \citep{thorbjarnason2023}. The main challenge for these models is that they do not scale well with larger network architectures or with increasing amounts of data. \\

\noindent In this thesis, I investigate whether it is possible to use local search (LS) to train BNNs and TNNs. Local search algorithms iteratively explore the solution space by making small, incremental changes to improve the solution. The primary objectives of this research are to develop effective local search strategies for training discrete neural networks, evaluate their scalability with larger datasets and larger network architectures. The discrete nature of the weights, combined with the binary activation function, makes it possible to implement an efficient delta evaluation, allowing many moves to be evaluated simultaneously. This makes it possible to train larger networks and use more data compared to the aforementioned MIP models. \\

\noindent The results I achieved show that the BeMi ensemble introduced by \citep{ambrogio2023} works very well in a few-shot learning context reaching a mean test accuracy of 86.84 \% and 77.38 \% on the MNIST and Fashion-MNIST datasets respectively using only 1,000 images from each dataset. Furthermore, my results indicate that the proposed aggregation algorithm, which only changes the values of weights based on the delta values for several batches, outperforms algorithms where weight changes are based on single batches. This works especially well for larger network architectures in combination with the suggested integer objective function, which does not overfit as much as the more common cross-entropy loss function. \\

\noindent In summary, this research demonstrates the potential of using local search for training BNNs and TNNs. In particular, local search is more scalable with respect to large datasets and network architecture than MIP models. Future work could explore further optimizations and extensions to other types of neural networks, such as convolutional neural networks (CNNs). 