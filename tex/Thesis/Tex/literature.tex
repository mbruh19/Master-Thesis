\section{Literature Review}

Deep learning (DL), a subset of machine learning (ML), has proven to be successfull in a large variety of tasks. Since \cite{imagenet2012} won the ImageNet competition using deep convolutional neural networks (CNNs), the research and interest in DL has exploded. In the following years several important advances were made within DL research. The introduction of generative adversarial networks (GANs) by \cite{goodfellow2014} revolutionized generative artificial intelligence (AI) and Google's DeepMind team developed AlphaGo using deep reinforcement learning (RL). Their agent defeated a world champion Go player, demonstrating the power of combining deep learning with other AI techniques. One of the most influential papers, 'Attention is All You need' by \cite{attention2017}, which introduced the transformer architecture. Until this, sequence-to-sequence models relied on recurrent neural networks (RNNs) and their variants like Long Short-Term Memory (LSTM), which processed sequences sequentially. As a consequence the training and inference were slow and difficult to parallelize. The transformer architecture is built upon a self-attention mechanism and it proved to be an important ingredience in natural language processing models. Beyond the already mentioned tasks where DL is used, it is also used in speech recognition, finance and healthcare. \\

\noindent Before the introduction of binary neural networks (BNNs) by \cite{Hubara2016} deep neural networks (DNNs) were primarily trained on graphic processing units (GPUs) using stochactic gradient descent (SGD) methods on enormous amounts of training data, which makes the training process computationally demanding. \cite{Hubara2016} introduced BNNs in which the weights and activations are restricted to the values +1 and -1 in order to come up with DNNs that could run on low-power devices. The restriction on the weights and activations makes it possible to represent them in 1-bits as 0 can be interpreted as -1. This means that the normal 32-bit floating points can be replaced by single bits, greatly reducing the memory usage. Another consequence of this is that the computationally demanding 32-bit floating point multiplication can be replaced by 1-bit XNOR-count operations, which could have a big impact on dedicated hardware. \\

\noindent Since \cite{Hubara2016} showed that BNNs could achieve state-of-the art results, much research has been conducted within the area. Large-weight models such as the AlexNet model, that won the ImageNet competition \citep{imagenet2012}, contains 60 million floating-point parameters and 650,000 neurons, making it difficult to deploy on devices with limited ressources. \cite{yuan2023} gives a great comprehensive review of BNN research and note that 1-bit values can theoretically have 32 times lower memory storage and 58 times faster inference speed than traditional 32-bit CNNs. Most of the research within BNNs is still concentrated within traditional NN training using SGD. In their review, \cite{yuan2023} divide the optimization solutions of BNNs into 5 areas. Quantization error minimization are methods that try to minimize the information loss during the transformation from 32-bit values to 1-bit values. Another area is loss function improvement, where a special loss or regularization can be added. A third area of research is gradient approximation, where researchers try to solve the zero derivative issue. A different way to optimize BNNs is to optimize BNN by optimizing the network architecture. The remaining area is called training strategy and tricks in their paper. In this thesis I will not dive deeper into those threads of research, but will instead attack the problem of optimizing BNNs from a different angle.\\

\noindent The introduction of BNNs is interesting not only for the ML community, but also for the operations research (OR) community. Training BNNs can be seen as a discrete optimization problem. BNNs are mostly trained using traditional SGD methods, but where the weights are binarized in the forward pass. It is possible, however, to train BNNs using constraint programming (CP) and mixed-integer programming (MIP). These model-based approaches have stronger convergence guarantees than Gradient Descent (GD) \citep{icarte2019}, but face other problems. A particular constraint is that MIP-models do not scale to large datasets as the model size depends on the size of the training set. Further, solutions found by MIP and CP that are proved to have optimal training error are likely to overfit the data and does not generalize well. \\

\noindent To deal with the problem of scalability, the research in training BNNs using model-based approaches are mostly focused on few-shot learning, where the size of the training set is very limited. This is itself interesting because there might be cases where the collection of large datasets are either impossible or very costly. Such cases could arise in areas as healthcare, where there in some cases is a limited amount of labelled data available \citep{ching2017}. It should be noted, that there is no consensus on the definition of a BNN. Some use the term for NNs in which the weights and activations are restricted to +1 and -1 and this is the notation that I will use as well. Others call it a BNN even though they allow weights to have a value of 0, meaning that the corresponding link can be removed from the network. This is also called a ternary neural network (TNN), which I will use to distinguish between these two types of networks. \\

\noindent \cite{icarte2019} show that their TNNs correctly classify up to 3 times more unseen examples compared to TNNs learned by GD when trained with few-shot learning. As a baseline GD model they extend the model from \cite{Hubara2016}, such that it also allows weights to be zero. To adress the problem of overfitting on the training data, the models developed have objective functions that encourage simplicity and robustness, as these are two well-known ML principles for generalization. In the work of \cite{icarte2019}, they force the training set to be correctly classified by introducing constraints ensuring this. As a result they can introduce objective functions that do not need to take into account how to maximize the training accuracy as this are automatically ensured by the constraints. As a measure for simplicity, they use the number of active connections, i.e. weights with values different from zero, which they try to minimize. Measuring robustness is somewhat more difficult, but they model this by the margins in each neuron. The margin of a neuron is defined to be the minimum absolute value of its preactivation. Larger margins require bigger changes on their inputs and weights to change the activation values and therefore help making the network robust. \\

\noindent They introduces three CP models - one without an robustness objective function, one that tries to minimize the number of connections in the network and one that seek to maximize the margins in the network. Two MIP models with these robustness objectives are also introduced as well as four hybrid models that combine the two approaches. For the hybrid models CP was used to initially find a feasible solution, which could then be given to the chosen MIP model. They evaluated the performance on the well-known benchmark dataset MNIST, in which the task is to classify digits. The used a balanced training set of 1 to 10 examples for each of the 10 classes. As such the size of the training set is ranging from 10 to 100. Even though they used a time-limit of two hours, their MIP models struggle to find solutions. Their best performing model, one of their hybrid models, reaches an testing accuracy of 56 \% with only 100 training examples and 2 hidden layers, each with 16 neurons. \\

\noindent \cite{thorbjarnason2023} see a potential in training NNs with MIP solvers. Unlike traditional state-of-the-art methods to train NNs, which require significant data, GPUs and extensive hyper-parameter tuning, MIP solvers do not need GPUs nor the same amount of hyper-parameter tuning. They recognize, however, that MIP models only can handle small amounts of data. For this reason, they do not expect NN training with MIP solvers to be competitive with gradient-based methods. According to them, the potential in MIP solvers is to train smaller networks with small batches of data. \\

\noindent The models trained by \cite{thorbjarnason2023} are integer neural networks (INNs), in which the weights can take any integer value in the interval $ \{-P, \ldots, P \}$. Their base model are built upon the model from \cite{icarte2019} and are then modified to test three different models - max-correct, min-hinge and sat-margin. While max-correct only seek to maximize the number of correct predictions of training samples, it does not aim to make the predictions as confident as possible. Max-correct simply optimizes a sum of binary variables, where each variable indicate if the corresponding training sample is correctly predicted. The min-hinge model also aim to make correct predictions, but here the objective function is made in such a way that an optimal solution makes confident predictions. The min-hinge is inspired by the squared hinge loss, and in their model they approximate it using a piecewise linear function. Their last model, sat-margin, combines aspects from both models. Again, it maximizes a sum of binary variables, but contrary to max-correct, the binary variables can only be 1 if the prediction is above a certain margin. \\

\noindent Besides from solving the classification problem, they also aim to find the minimum number of neurons needed in the NN to fit the training data. To do this, they propose yet another objective function, which can be added to their max-correct and sat-margin model. The model compression objective function is to minimize a sum of binary variables, one for each neuron in all the hidden layers. This binary variable can only be 1 if all the weights going into the neuron and all the weights going out of the neuron are 0. This is a way of compressing the model and it is an advantage of using discrete optimization solvers, that makes it possible to simultaneously train the NN and optimize its architecture. \\

\noindent Another important contribution of their work is their batch training algorithm that increases the amount of data that can be used to train their NN significantly. The way it works are, in short terms, as follows: Start by distributing training data into small batches and train a MIP NN model on each batch independently of each other. When a model for each batch has been trained, all the NNs are combined into a single NN, where validation accuracies are used as weights to make it a weighted average. To ensure convergence, they then constrain the bounds of the weights before they repeat the process, which is then repeated until convergence. After convergence the model with the highest validation accuracy is chosen as the solution. It should be noted that the training of the MIP NNs in each iteration can be done in parallel. \\

\noindent \cite{ambrogio2023} introduce yet another approach to train both TNNs and INNs. Instead of training a single model for a classification problem, they use an ensemble approach, which is based on training a single NN for each possible pair of classes. After training, a majority voting scheme is used to predict the final output. Further, in the training of a NN, they suggest to train it by solving a lexicographic multi-objective MIP model. Their multi-objective model is actually quite intuitive. As a starting point, they start training their model to maximize the number of confidently correctly predicted data. The solution found in that model is then used as a warm start in a model that maximizes the margins to make the network more robust. Finally, the solution of that model is used as a warm start in a model that minimizes the number of connections. Notice, that the models in a way inherit constraints, such that the second model must still be able to predict the training data confidently and the third model must satisfy the margins that the second model maximized. \\

\noindent Their ensemble approach is quite different from the other approaches. Instead of training a single network with a neuron in the last layer for each possible label, they need to train a network for each pair of classes. For each pair of classes they then solve a binary classification problem. A sample from the test dataset is then fed into all of the networks and a majority voting system is then used to determine the final output. For the MNIST dataset and with a balanced training dataset of 100 samples, their average testing accuracy is at 68 \%. For this experiment they trained each of their NNs for 160 seconds, given a total training time of two hours, which could be greatly reduced in wall-clock runtime by training their networks in parallel.  As architecture for their NNs they use the structure $[784, 4, 4, 1]$ or $[784, 10, 3, 1$]. They also investigate how their approach scales with the number of training images. As such they train their NNs on also 200, 300 and 400 training samples, but this time with 600 seconds allocated to each NN, which gives a total wall-clock runtime of 7.5 hours, when trained one by one. Their testing accuracy increases to a maximum of 81 \% with 400 training samples. \\

\noindent 

