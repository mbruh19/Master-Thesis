\section{Literature Review}

DL, a subset of machine learning (ML), has proven to be successful in a large variety of tasks. Since \cite{imagenet2012} won the ImageNet competition using deep CNNs, the research and interest in DL has exploded. In the following years, several important advances were made within DL research. The introduction of generative adversarial networks (GANs) by \cite{goodfellow2014} revolutionized generative artificial intelligence (AI) and Google's DeepMind team developed AlphaGo using deep reinforcement learning (RL). Their agent defeated a world champion Go player, demonstrating the power of combining deep learning with other AI techniques. One of the most influential papers published is the 'Attention is All You need' paper by \cite{attention2017}, which introduced the transformer architecture. This paper was originally published in 2017 and was then republished in 2023. Until this, sequence-to-sequence models relied on recurrent neural networks (RNNs) and their variants like Long Short-Term Memory (LSTM), which processed sequences sequentially. As a consequence, the training and inference were slow and difficult to parallelize. The transformer architecture is built upon a self-attention mechanism, and it proved to be an important ingredient in natural language processing models. Beyond the already mentioned tasks where DL is used, it is also used in speech recognition, finance, and healthcare. \\

\noindent Before the introduction of BNNs by \cite{Hubara2016} deep neural networks (DNNs) were primarily trained on graphics processing units (GPUs) using stochastic gradient descent (SGD) methods on enormous amounts of training data, which made the training process computationally demanding. \cite{Hubara2016} introduced BNNs in which the weights and activations are restricted to the values +1 and -1 to create DNNs that could run on low-power devices. The restriction on the weights and activations makes it possible to represent them in 1-bits as 0 can be interpreted as -1. This means that the normal 32-bit floating points can be replaced by single bits, greatly reducing the memory usage. Another consequence of this is that the computationally demanding 32-bit floating point multiplication can be replaced by 1-bit XNOR-count operations, which can have a big impact on dedicated hardware. \\

\noindent Since \cite{Hubara2016} showed that BNNs could achieve state-of-the art results, much research has been conducted within the area. Large-weight models such as the AlexNet model that won the ImageNet competition \citep{imagenet2012}, contain 60 million floating-point parameters and 650,000 neurons, making it difficult to deploy on devices with limited resources. \cite{yuan2023} provides a comprehensive review of BNN research and note that 1-bit values can theoretically require 32 times less memory storage and offer 58 times faster inference speed than traditional 32-bit CNNs. Most of the research within BNNs is still concentrated within traditional NN training using SGD. In their review, \cite{yuan2023} divide the optimization solutions of BNNs into five areas. Quantization error minimization includes methods that try to minimize the information loss during the transformation from 32-bit values to 1-bit values. Another area is loss function improvement, where a special loss or regularization can be added. A third area of research is gradient approximation, where researchers try to solve the zero derivative issue. A different way to optimize BNNs is to optimize the network architecture. The remaining area is called training strategy and tricks in their paper. In this thesis I will not dive deeper into those threads of research, but will instead address the problem of training BNNs from a different angle.\\

\noindent The introduction of BNNs is interesting not only for the ML community, but also for the operations research (OR) community. Training BNNs can be seen as a discrete optimization problem. BNNs are mostly trained using traditional SGD methods but with the weights binarized in the forward pass. It is possible, however, to train BNNs using constraint programming (CP) and MIP. These model-based approaches have stronger convergence guarantees than Gradient Descent (GD) \citep{icarte2019}, but face other problems. A particular constraint is that MIP-models do not scale to large datasets as the model size depends on the size of the training set. Further, solutions found by MIP and CP that are proved to have optimal training error are likely to overfit the data and do not generalize well. \\

\noindent To deal with the problem of scalability, the research in training BNNs using model-based approaches is mostly focused on few-shot learning, where the size of the training set is very limited. This is itself interesting because there might be cases where the collection of large datasets is either impossible or very costly. Such cases could arise in areas such as healthcare, where there is in some cases a limited amount of labeled data available \citep{ching2017}. It should be noted that there is no consensus on the definition of a BNN. Some use the term for NNs in which the weights and activations are restricted to +1 and -1, and this is the notation that I will use as well. Others call it a BNN even though they allow weights to have a value of 0, meaning that the corresponding link can be removed from the network. This is also called a ternary neural network (TNN), which I will use to distinguish between these two types of networks. \\

\noindent \cite{icarte2019} show that their TNNs correctly classify up to three times more unseen examples compared to TNNs learned by GD when trained with few-shot learning. As a baseline GD model, they extend the model from \cite{Hubara2016}, such that it also allows weights to be zero. To address the problem of overfitting on the training data, the models developed have objective functions that encourage simplicity and robustness, as these are two well-known ML principles for generalization. In the work of \cite{icarte2019}, they force the training set to be correctly classified by introducing constraints to ensure this. As a result, they can introduce objective functions that do not need to take into account how to maximize the training accuracy, as this is automatically ensured by the constraints. As a measure for simplicity, they use the number of active connections, i.e. weights with values different from zero, which they try to minimize. Measuring robustness is somewhat more difficult, but they model this by the margins in each neuron. The margin of a neuron is defined to be the minimum absolute value of its preactivation. Larger margins require bigger changes on their inputs and weights to change the activation values and therefore help make the network robust. \\

\noindent They introduce three CP models - one without a robustness objective function, one that tries to minimize the number of connections in the network, and one that seeks to maximize the margins in the network. Two MIP models with these robustness objectives are also introduced, as well as four hybrid models that combine the two approaches. For the hybrid models, CP was used to initially find a feasible solution, which could then be given to the chosen MIP model. They evaluated the performance on the well-known benchmark dataset MNIST, in which the task is to classify digits. They used a balanced training set of 1 to 10 examples for each of the 10 classes. As such, the size of the training set ranges from 10 to 100. Even though they used a time-limit of two hours, their MIP models struggled to find solutions. Their best performing model, one of their hybrid models, reached an testing accuracy of 56 \% with only 100 training examples and two hidden layers, each with 16 neurons. \\

\noindent \cite{thorbjarnason2023} see a potential in training NNs with MIP solvers. Unlike traditional state-of-the-art methods for training NNs, which require significant data, GPUs, and extensive hyper-parameter tuning, MIP solvers do not need GPUs or the same amount of hyper-parameter tuning. They recognize, however, that MIP models can only handle small amounts of data. For this reason, they do not expect NN training with MIP solvers to be competitive with gradient-based methods. According to them, the potential of MIP solvers is to train smaller networks with small batches of data. \\

\noindent The models trained by \cite{thorbjarnason2023} are integer neural networks (INNs), in which the weights can take any integer value in the interval $ \{-P, \ldots, P \}$. Their base model is built upon the model from \cite{icarte2019} and is then modified to test three different models - max-correct, min-hinge, and sat-margin. While max-correct only seeks to maximize the number of correct predictions of training samples, it does not aim to make the predictions as confident as possible. Max-correct simply optimizes a sum of binary variables, where each variable indicates if the corresponding training sample is correctly predicted. The min-hinge model also aims to make correct predictions, but here the objective function is designed in such a way that an optimal solution ensures confident predictions. The min-hinge model is inspired by the squared hinge loss, and the authors approximate it using a piecewise linear function. Their last model, sat-margin, combines aspects from both previous models. Again, it maximizes a sum of binary variables, but contrary to max-correct, the binary variables can only be set to 1 if the prediction is above a certain margin. \\

\noindent Aside from solving the classification problem, they also aim to find the minimum number of neurons needed in the NN to fit the training data. To do this, they propose yet another objective function, which can be added to their max-correct and sat-margin models. The model compression objective function is to minimize a sum of binary variables, one for each neuron in all the hidden layers. This binary variable can only be set to 1 if all the weights going into the neuron and all the weights going out of the neuron are 0. This is a way of compressing the model, and it is an advantage of using discrete optimization solvers that makes it possible to simultaneously train the NN and optimize its architecture. \\

\noindent Another important contribution of their work is their batch training algorithm that increases the amount of data that can be used to train their NN significantly. The way it works is, in short terms, as follows: start by distributing training data into small batches and training a MIP NN model on each batch independently of each other. When a model for each batch has been trained, all the NNs are combined into a single NN, using validation accuracies as weights to create a weighted average. To ensure convergence, they then constrain the bounds of the weights before they repeat the process, which is then repeated until convergence. After convergence, the model with the highest validation accuracy is chosen as the solution. It should be noted that the training of the MIP NNs in each iteration can be done in parallel. \\

\noindent \cite{ambrogio2023} introduce yet another approach to train both TNNs and INNs. Instead of training a single model for a classification problem, they use an ensemble approach, which is based on training a single NN for each possible pair of classes. After training, a majority voting scheme is used to predict the final output. Further, in the training of a NN, they suggest training it by solving a lexicographic multi-objective MIP model. Their multi-objective model is actually quite intuitive. As a starting point, they start by training their model to maximize the number of confidently correctly predicted training examples. The solution found in that model is then used as a warm start in a model that maximizes the margins to make the network more robust. Finally, the solution of that model is used as a warm start in a model that minimizes the number of connections. Note, that the models in a way inherit constraints, such that the second model must still be able to predict the training data confidently, and the third model must satisfy the margins that the second model maximized. \\

\noindent Their ensemble approach is quite different from the other approaches. Instead of training a single network with a neuron in the last layer for each possible label, they train a network for each pair of classes. For each pair of classes, they then solve a binary classification problem. A sample from the test dataset is then fed into all of the networks, and a majority voting system is then used to determine the final output. For the MNIST dataset and with a balanced training dataset of 100 samples, their average testing accuracy is at 68 \%. For this experiment, they trained each of their NNs for 160 seconds, resulting in a total training time of two hours, which could be greatly reduced in wall-clock runtime by training their networks in parallel.  As an architecture for their NNs, they use the structure $[784, 4, 4, 1]$ or $[784, 10, 3, 1$]. They also investigate how their approach scales with the number of training images. As such, they train their NNs on also 200, 300, and 400 training samples, but this time with 600 seconds allocated to each NN, which results in a total wall-clock runtime of 7.5 hours, when trained one by one. Their testing accuracy increases to a maximum of 81 \% with 400 training samples. \\


